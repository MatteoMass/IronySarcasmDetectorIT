{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70aa0333-9475-4e45-8371-4a4d73bed2d7",
   "metadata": {},
   "source": [
    "# Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43fecb6-41cd-4639-b7bf-f6b7b87bbe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import Config\n",
    "import keras\n",
    "\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "EMBEDDING_SIZE = 128\n",
    "INPUT_LENGHT = 194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aaf8446-c239-4007-a6fc-b0e188b64c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.tokenizationBagWords import tokenize_frame\n",
    "from keras.layers import Conv1D, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9436aaf-71ed-4222-8166-6be88591b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [[(128, True, 0.5), (64, True, 0.2), (16, False, 0.0)],\n",
    " [(128, True, 0.5), (32, False, 0.2)],\n",
    " [(128, True, 0.5), (64, False, 0.2)],  \n",
    " [(128, True, 0.0), (32, False, 0.0)],\n",
    " [(128, False, 0.0)],\n",
    " [(128, False, 0.2)]\n",
    " ]\n",
    "loss = ['binary_crossentropy']\n",
    "epochs = [10]\n",
    "\n",
    "\n",
    "combinations = list(itertools.product(*[hidden_layers, loss, epochs]))\n",
    "combinations\n",
    "\n",
    "validation_performance = []\n",
    "\n",
    "training_data = pd.read_csv(Config.TRAINING_DATASET_PATH)\n",
    "\n",
    "from preprocessing.pipeline import ItalianTweetsPreprocessingPipeline\n",
    "pp = ItalianTweetsPreprocessingPipeline()\n",
    "\n",
    "preprocessed_training_data = pp.apply(training_data)\n",
    "\n",
    "x = preprocessed_training_data['text']\n",
    "y_1 = list(preprocessed_training_data['irony'])\n",
    "y_2 = list(preprocessed_training_data['sarcasm'])\n",
    "y = list(zip(y_1, y_2))\n",
    "\n",
    "x, _ , num_words = tokenize_frame(x)\n",
    "\n",
    "\n",
    "tmp = list(zip(x,y))\n",
    "random.shuffle(tmp)\n",
    "x, y = zip(*tmp)\n",
    "\n",
    "\n",
    "validate_size = int(len(x)*VALIDATION_SPLIT)\n",
    "x_train, x_validate = x[:-validate_size], x[-validate_size:]\n",
    "x_train = np.asarray(x_train)\n",
    "x_validate = np.asarray(x_validate)\n",
    "y_train, y_validate = y[:-validate_size], y[-validate_size:]\n",
    "y_train = np.asarray(y_train)\n",
    "y_validate = np.asarray(y_validate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16bdc6c-abfe-42a4-8282-21035c678a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 Configurazione: ([(128, True, 0.5), (64, True, 0.2), (16, False, 0.0)], 'binary_crossentropy', 10)\n",
      "13/13 [==============================] - 1s 47ms/step - loss: 0.5779 - accuracy: 1.0000\n",
      "index: 1 Configurazione: ([(128, True, 0.5), (32, False, 0.2)], 'binary_crossentropy', 10)\n",
      "13/13 [==============================] - 0s 36ms/step - loss: 0.5942 - accuracy: 1.0000\n",
      "index: 2 Configurazione: ([(128, True, 0.5), (64, False, 0.2)], 'binary_crossentropy', 10)\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.5993 - accuracy: 1.0000\n",
      "index: 3 Configurazione: ([(128, True, 0.0), (32, False, 0.0)], 'binary_crossentropy', 10)\n",
      "13/13 [==============================] - 1s 42ms/step - loss: 0.5847 - accuracy: 1.0000\n",
      "index: 4 Configurazione: ([(128, False, 0.0)], 'binary_crossentropy', 10)\n",
      "13/13 [==============================] - 0s 30ms/step - loss: 0.6001 - accuracy: 1.0000\n",
      "index: 5 Configurazione: ([(128, False, 0.2)], 'binary_crossentropy', 10)\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.5981 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "index = 0\n",
    "for combination in combinations:\n",
    "    print(\"index: \" + str(index) + \" Configurazione: \" + str(combination))\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Embedding(num_words, EMBEDDING_SIZE, input_length=INPUT_LENGHT, mask_zero=True))\n",
    "    for e in combination[0]:\n",
    "        model.add(keras.layers.GRU(e[0], return_sequences=e[1]))\n",
    "        if e[2] != 0.0:\n",
    "            model.add(keras.layers.Dropout(e[2]))\n",
    "    model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    "    model.compile(loss=combination[1], optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=combination[2], verbose=0)\n",
    "    result = model.evaluate(x_validate, y_validate)\n",
    "    results.append([combination, dict(zip(model.metrics_names, result))])\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4cad242-4b71-4480-b245-b69f3832a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameter\n",
      "([(128, True, 0.5), (64, True, 0.2), (16, False, 0.0)], 'binary_crossentropy', 10) 1.0\n",
      "([(128, True, 0.5), (32, False, 0.2)], 'binary_crossentropy', 10) 1.0\n",
      "([(128, True, 0.5), (64, False, 0.2)], 'binary_crossentropy', 10) 1.0\n",
      "([(128, True, 0.0), (32, False, 0.0)], 'binary_crossentropy', 10) 1.0\n",
      "([(128, False, 0.0)], 'binary_crossentropy', 10) 1.0\n",
      "([(128, False, 0.2)], 'binary_crossentropy', 10) 1.0\n"
     ]
    }
   ],
   "source": [
    "metric = 'accuracy'\n",
    "values = []\n",
    "\n",
    "\n",
    "for i in range(len(results)):\n",
    "    values.append((results[i][0],results[i][1][metric]))\n",
    "                  \n",
    "values.sort(key=lambda tup: tup[1], reverse = True)\n",
    "\n",
    "print('Best Hyperparameter')\n",
    "for v in values:\n",
    "    print(v[0], v[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b11ac-3633-4b09-9f4b-ff7a8729ed04",
   "metadata": {},
   "source": [
    "# Best Modelfrom pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a325b-54b1-430e-8607-9d840b93a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "\n",
    "from config import Config\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816a2d2-4bfc-4e94-8146-a1b5b9a40c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(Config.TRAINING_DATASET_PATH)\n",
    "test_data = pd.read_csv(Config.TEST_DATASET_PATH)\n",
    "del test_data['id']\n",
    "del test_data['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90edbe-c197-4e2e-8b41-a89bc11b8c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.pipeline import ItalianTweetsPreprocessingPipeline\n",
    "pp = ItalianTweetsPreprocessingPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca1f6f-6a6b-4694-8d87-1f50cb122c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_training_data = pp.apply(training_data)\n",
    "preprocessed_test_data = pp.apply(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f73d6b-1f3d-4bf2-be6f-ea90fbb0e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocessed_training_data['text']\n",
    "x_test = preprocessed_test_data['text']\n",
    "y_train = preprocessed_training_data[['irony','sarcasm']]\n",
    "y_test = preprocessed_test_data[['irony','sarcasm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98717461-711a-4008-a96f-9049bcf3ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tk\n",
    "from tensorflow import keras\n",
    "from config import Config\n",
    "\n",
    "INPUT_LENGHT = 194\n",
    "EMBEDDING_SIZE = 128\n",
    "VALIDATION_SIZE = 0.1\n",
    "\n",
    "from preprocessing.tokenizationBagWords import tokenize_frame\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec8a28-4ab1-4bdf-97c9-4f02ea66a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "seed_value= 450\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions: \n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35932e79-d17a-4eb6-ad53-c6d6bcfe6a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_gru(x, y, num_words: int, combination, embedding_size=EMBEDDING_SIZE):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Embedding(num_words, EMBEDDING_SIZE, input_length=INPUT_LENGHT, mask_zero=True))\n",
    "    for e in combination[0]:\n",
    "        model.add(keras.layers.GRU(e[0], return_sequences=e[1]))\n",
    "        if e[2] != 0.0:\n",
    "            model.add(keras.layers.Dropout(e[2]))\n",
    "    model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    "    model.compile(loss=combination[1], optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x, y, epochs=combination[2], validation_split = VALIDATION_SIZE)\n",
    "    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2235e79a-2167-406f-a95a-3f4b9913cbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "112/112 [==============================] - 41s 368ms/step - loss: 0.6241 - accuracy: 0.9958 - val_loss: 0.6135 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 40s 355ms/step - loss: 0.6167 - accuracy: 1.0000 - val_loss: 0.6113 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 40s 353ms/step - loss: 0.6139 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 46s 410ms/step - loss: 0.6111 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 44s 393ms/step - loss: 0.6098 - accuracy: 0.9997 - val_loss: 0.6088 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 41s 365ms/step - loss: 0.6070 - accuracy: 0.9997 - val_loss: 0.6098 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 41s 368ms/step - loss: 0.6039 - accuracy: 0.9997 - val_loss: 0.6162 - val_accuracy: 0.9975\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 39s 350ms/step - loss: 0.6011 - accuracy: 0.9997 - val_loss: 0.6043 - val_accuracy: 0.9975\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 41s 370ms/step - loss: 0.5979 - accuracy: 0.9997 - val_loss: 0.6029 - val_accuracy: 0.9975\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 41s 366ms/step - loss: 0.5902 - accuracy: 0.9997 - val_loss: 0.6175 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "x_train_gru, x_test_gru, num_words = tokenize_frame(x_train.tolist(), x_test.tolist())\n",
    "\n",
    "x_train_gru = np.asarray(x_train_gru)\n",
    "x_test_gru = np.asarray(x_test_gru)\n",
    "\n",
    "hyperparameters = [[(128, True, 0.5), (64, True, 0.2), (16, False, 0.0)], 'binary_crossentropy', 10]\n",
    "model = model_gru(x_train_gru, y_train, num_words, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3317a55b-682b-4dea-b1bb-398a88b0b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Average Task A-B\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5480795538251079, 0.3123566819848675]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from custom_metrics import computePerformanceTaskB_2output\n",
    "\n",
    "print(\"F1 Average Task A-B\")\n",
    "computePerformanceTaskB_2output(model, x_test_gru, y_test, y_test['irony'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7f842-6508-4843-8a03-4c9d7abbb652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
